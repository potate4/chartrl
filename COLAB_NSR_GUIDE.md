# NSR Training on Google Colab - Complete Guide

**Complete guide for running PSR, NSR, and W-REINFORCE training on Google Colab.**

---

## ‚ö†Ô∏è Important: GPU Requirements

**You NEED Colab Pro or Pro+ for A100 GPU!**

| Colab Plan | GPU Available | Can Run NSR? | Cost |
|------------|---------------|--------------|------|
| **Free** | T4 (16GB) | ‚ùå NO (OOM) | $0 |
| **Colab Pro** | A100 40GB | ‚úÖ YES | $10-20/month |
| **Colab Pro+** | A100 40GB | ‚úÖ YES | $50/month (more hours) |

**For free option:** Use Kaggle 2x T4 instead (see KAGGLE_2xT4_GUIDE.md)

---

## üìã Table of Contents

1. [Quick Setup (5 minutes)](#quick-setup-5-minutes)
2. [Training Commands](#training-commands)
3. [Pass@k Evaluation](#passk-evaluation)
4. [Time & Cost Estimates](#time--cost-estimates)
5. [Troubleshooting](#troubleshooting)

---

## üöÄ Quick Setup (5 minutes)

### Step 1: Get Colab Pro A100

1. Go to https://colab.research.google.com/
2. Subscribe to Colab Pro ($10-20/month)
3. Create new notebook
4. **Runtime ‚Üí Change runtime type ‚Üí A100 GPU**

### Step 2: Install Dependencies (2 minutes)

```python
# Cell 1: Install requirements
!pip install -q torch==2.4.0
!pip install -q transformers==4.47.1
!pip install -q peft==0.14.0
!pip install -q trl==0.12.1
!pip install -q accelerate==1.2.1
!pip install -q deepspeed==0.15.4
!pip install -q datasets==3.1.0
!pip install -q qwen-vl-utils==0.0.8
!pip install -q wandb
!pip install -q sentence-transformers
!pip install -q sacrebleu
!pip install -q matplotlib
!pip install -q numpy
```

**Time:** ~2 minutes

### Step 3: Clone Repository (1 minute)

```python
# Cell 2: Clone your code
!git clone https://github.com/yourusername/chartrl.git
%cd chartrl
```

**Or upload files manually:**
```python
# Cell 2: Upload files
from google.colab import files
import os

# Create directory structure
!mkdir -p trainers
!mkdir -p evaluation

# Upload these files:
# - main.py
# - grpo_utils.py
# - dataset_process.py
# - models.py
# - prompts.py
# - metrics.py
# - utils.py
# - deepspeed_zero3.yaml
# - trainers/psr_trainer.py
# - trainers/nsr_trainer.py
# - trainers/weighted_reinforce_trainer.py
# - evaluation/pass_at_k.py
# - evaluation/plotting.py
# - generate_samples.py
```

### Step 4: Setup Accelerate (1 minute)

```python
# Cell 3: Create DeepSpeed config
config_content = """compute_environment: LOCAL_MACHINE
debug: false
deepspeed_config:
  deepspeed_multinode_launcher: standard
  offload_optimizer_device: none
  offload_param_device: none
  zero3_init_flag: true
  zero3_save_16bit_model: true
  zero_stage: 3
distributed_type: DEEPSPEED
downcast_bf16: 'no'
machine_rank: 0
main_training_function: main
mixed_precision: bf16
num_machines: 1
num_processes: 1
rdzv_backend: static
same_network: true
tpu_env: []
tpu_use_cluster: false
tpu_use_sudo: false
use_cpu: false
main_process_port: 29501
"""

with open('deepspeed_zero3.yaml', 'w') as f:
    f.write(config_content)

print("‚úì DeepSpeed config created")
```

**Time:** ~30 seconds

### Step 5: Login to Weights & Biases (Optional, 1 minute)

```python
# Cell 4: Login to W&B for tracking (optional)
!pip install -q wandb
import wandb

# Login with your API key
wandb.login()

# Or skip tracking:
# import os
# os.environ['WANDB_MODE'] = 'disabled'
```

**Time:** ~1 minute

---

## üéØ Training Commands

All commands assume you're in the chartrl directory.

### Configuration 1: GRPO Baseline (Comparison)

```python
# Cell 5: GRPO Baseline Training
!accelerate launch --config_file=deepspeed_zero3.yaml main.py \
  --mode grpo \
  --vlm-name qwen2-5-3b \
  --dataset-name evochart \
  --seed 2025 \
  --subset-size 10000 \
  --num-epochs 1 \
  --num-generations 2 \
  --batch-size 2 \
  --disable-gradient-checkpointing
```

**Expected Output:**
```
GRPO TRAINING CONFIGURATION:
  Training mode: grpo
  Training samples: 10000
  Epochs: 1
  Batch size: 2
  Generations per sample: 2
  Gradient checkpointing: False

trainable params: 18,576,384 || all params: 3,773,199,360 || trainable%: 0.4923

Training progress:
  0% 10/5000 [08:30<12:45:19, 52.66s/it]
Format rewards: [2.0, 0.0, 2.0, 0.0]
Rewards Accuracy: [1.0, 0.0, 1.0, 1.0]
...
```

**Time:** ~18-21 hours on A100 40GB
**Cost:** ~$2-3 (Colab Pro)
**Checkpoint:** `grpo-start-ckpts/qwen2-5-3b-prm-large-train-v2-2025/`

---

### Configuration 2: PSR Training (Best Pass@1)

```python
# Cell 6: PSR Training
!accelerate launch --config_file=deepspeed_zero3.yaml main.py \
  --mode psr \
  --vlm-name qwen2-5-3b \
  --dataset-name evochart \
  --seed 2025 \
  --subset-size 10000 \
  --num-epochs 1 \
  --num-generations 2 \
  --batch-size 2 \
  --reward-threshold 0.5 \
  --disable-gradient-checkpointing
```

**Expected Output:**
```
PSR TRAINING CONFIGURATION:
  Training mode: psr
  Training samples: 10000
  Epochs: 1
  Batch size: 2
  Generations per sample: 2
  Gradient checkpointing: False
  Reward threshold: 0.5

‚úì PSR reward filtering enabled
  ‚Üí Training only on POSITIVE samples (correct responses)
  ‚Üí Expected: High Pass@1, Lower Pass@k at large k

trainable params: 18,576,384 || all params: 3,773,199,360 || trainable%: 0.4923

Training progress:
PSR: Filtered 3/4 positive samples
...
```

**Time:** ~16-18 hours on A100 40GB (faster due to fewer samples)
**Cost:** ~$2-2.5 (Colab Pro)
**Checkpoint:** `grpo-start-ckpts/qwen2-5-3b-prm-large-train-v2-psr-2025/`

---

### Configuration 3: NSR Training (Best Diversity)

```python
# Cell 7: NSR Training
!accelerate launch --config_file=deepspeed_zero3.yaml main.py \
  --mode nsr \
  --vlm-name qwen2-5-3b \
  --dataset-name evochart \
  --seed 2025 \
  --subset-size 10000 \
  --num-epochs 1 \
  --num-generations 4 \
  --batch-size 2 \
  --reward-threshold 0.5 \
  --disable-gradient-checkpointing
```

**IMPORTANT:** NSR needs `--num-generations 4` or higher!

**Expected Output:**
```
NSR TRAINING CONFIGURATION:
  Training mode: nsr
  Training samples: 10000
  Epochs: 1
  Batch size: 2
  Generations per sample: 4
  Gradient checkpointing: False
  Reward threshold: 0.5

‚úì NSR reward filtering enabled
  ‚Üí Training only on NEGATIVE samples (incorrect responses)
  ‚Üí Expected: High Pass@k across all k, preserves diversity

trainable params: 18,576,384 || all params: 3,773,199,360 || trainable%: 0.4923

Training progress:
NSR: Filtered 3/4 negative samples
...
```

**Time:** ~22-24 hours on A100 40GB (4 generations = 2√ó slower)
**Cost:** ~$3-3.5 (Colab Pro)
**Checkpoint:** `grpo-start-ckpts/qwen2-5-3b-prm-large-train-v2-nsr-2025/`

---

### Configuration 4: W-REINFORCE Training (BEST OVERALL) ‚≠ê

```python
# Cell 8: W-REINFORCE Training (RECOMMENDED)
!accelerate launch --config_file=deepspeed_zero3.yaml main.py \
  --mode w-reinforce \
  --vlm-name qwen2-5-3b \
  --dataset-name evochart \
  --seed 2025 \
  --subset-size 10000 \
  --num-epochs 1 \
  --num-generations 4 \
  --batch-size 2 \
  --lambda-psr 0.1 \
  --reward-threshold 0.5 \
  --disable-gradient-checkpointing
```

**Expected Output:**
```
W-REINFORCE TRAINING CONFIGURATION:
  Training mode: w-reinforce
  Training samples: 10000
  Epochs: 1
  Batch size: 2
  Generations per sample: 4
  Gradient checkpointing: False
  Lambda PSR: 0.1
  Reward threshold: 0.5

‚úì W-REINFORCE reward filtering enabled
  ‚Üí Weighted training: 0.1¬∑PSR + NSR
  ‚Üí Expected: Best Pass@1 and Pass@k overall

trainable params: 18,576,384 || all params: 3,773,199,360 || trainable%: 0.4923

Training progress:
W-REINFORCE: 2 positive (Œª=0.1), 2 negative (Œª=1.0)
...
```

**Time:** ~22-24 hours on A100 40GB
**Cost:** ~$3-3.5 (Colab Pro)
**Checkpoint:** `grpo-start-ckpts/qwen2-5-3b-prm-large-train-v2-w-reinforce-2025/`

---

## üìä Pass@k Evaluation

After training, evaluate with Pass@k metrics.

### Step 1: Generate Samples (2-4 hours per method)

```python
# Cell 9: Generate samples for GRPO baseline
!python generate_samples.py \
  --checkpoint-path grpo-start-ckpts/qwen2-5-3b-prm-large-train-v2-2025 \
  --dataset-name evochart \
  --output-path samples_grpo.json \
  --num-samples 256 \
  --batch-size 4 \
  --max-problems 500
```

**Time:** ~2-4 hours (500 problems √ó 256 samples √ó 0.1s/sample)

```python
# Cell 10: Generate samples for PSR
!python generate_samples.py \
  --checkpoint-path grpo-start-ckpts/qwen2-5-3b-prm-large-train-v2-psr-2025 \
  --dataset-name evochart \
  --output-path samples_psr.json \
  --num-samples 256 \
  --batch-size 4 \
  --max-problems 500
```

**Time:** ~2-4 hours

```python
# Cell 11: Generate samples for NSR
!python generate_samples.py \
  --checkpoint-path grpo-start-ckpts/qwen2-5-3b-prm-large-train-v2-nsr-2025 \
  --dataset-name evochart \
  --output-path samples_nsr.json \
  --num-samples 256 \
  --batch-size 4 \
  --max-problems 500
```

**Time:** ~2-4 hours

```python
# Cell 12: Generate samples for W-REINFORCE
!python generate_samples.py \
  --checkpoint-path grpo-start-ckpts/qwen2-5-3b-prm-large-train-v2-w-reinforce-2025 \
  --dataset-name evochart \
  --output-path samples_w_reinforce.json \
  --num-samples 256 \
  --batch-size 4 \
  --max-problems 500
```

**Time:** ~2-4 hours

---

### Step 2: Compute Pass@k Metrics (1 minute per method)

```python
# Cell 13: Compute Pass@k for all methods
!python evaluation/pass_at_k.py \
  --samples-path samples_grpo.json \
  --output-path results_grpo.json \
  --method-name GRPO

!python evaluation/pass_at_k.py \
  --samples-path samples_psr.json \
  --output-path results_psr.json \
  --method-name PSR

!python evaluation/pass_at_k.py \
  --samples-path samples_nsr.json \
  --output-path results_nsr.json \
  --method-name NSR

!python evaluation/pass_at_k.py \
  --samples-path samples_w_reinforce.json \
  --output-path results_w_reinforce.json \
  --method-name "W-REINFORCE"
```

**Expected Output:**
```
Evaluating Pass@k on 500 problems...
  Pass@1: 0.6514 (65.14%)
  Pass@2: 0.7123 (71.23%)
  Pass@4: 0.7589 (75.89%)
  Pass@8: 0.7912 (79.12%)
  Pass@16: 0.8145 (81.45%)
  Pass@32: 0.8321 (83.21%)
  Pass@64: 0.8467 (84.67%)
  Pass@128: 0.8589 (85.89%)
  Pass@256: 0.8673 (86.73%)

‚úì Saved Pass@k results to results_nsr.json
```

**Time:** ~1 minute per method

---

### Step 3: Plot Comparison (30 seconds)

```python
# Cell 14: Plot Pass@k curves
!python evaluation/plotting.py \
  --results results_grpo.json results_psr.json results_nsr.json results_w_reinforce.json \
  --labels "GRPO" "PSR" "NSR" "W-REINFORCE" \
  --output comparison_pass_at_k.png \
  --title "Pass@k Comparison: Chart Reasoning"

# Display plot
from IPython.display import Image
Image('comparison_pass_at_k.png')
```

**Time:** ~30 seconds

---

### Step 4: Download Results

```python
# Cell 15: Download all results
from google.colab import files

# Download checkpoints (optional - large files)
# !zip -r checkpoints.zip grpo-start-ckpts/
# files.download('checkpoints.zip')

# Download results (small files)
files.download('results_grpo.json')
files.download('results_psr.json')
files.download('results_nsr.json')
files.download('results_w_reinforce.json')
files.download('comparison_pass_at_k.png')
```

---

## ‚è±Ô∏è Time & Cost Estimates

### Fast Configuration (10K samples, 1 epoch)

| Method | Generations | Training Time | Eval Time | Total Time | Cost (Colab Pro) |
|--------|------------|---------------|-----------|------------|------------------|
| **GRPO** | 2 | 18-21 hours | 2-4 hours | ~20-25 hours | $2.50-3.00 |
| **PSR** | 2 | 16-18 hours | 2-4 hours | ~18-22 hours | $2.00-2.50 |
| **NSR** | 4 | 22-24 hours | 2-4 hours | ~24-28 hours | $3.00-3.50 |
| **W-REINFORCE** | 4 | 22-24 hours | 2-4 hours | ~24-28 hours | $3.00-3.50 |

**Total for all 4 methods:** ~86-103 hours (~$10-12 if run sequentially)

**If run in parallel (4 Colab notebooks simultaneously):** ~24-28 hours (~$12-14)

---

### Moderate Configuration (34K samples, 1 epoch)

| Method | Generations | Training Time | Eval Time | Total Time | Cost (Colab Pro) |
|--------|------------|---------------|-----------|------------|------------------|
| **GRPO** | 2 | ~3 days | 6-8 hours | ~3.5 days | $8-10 |
| **PSR** | 2 | ~2.5 days | 6-8 hours | ~3 days | $7-9 |
| **NSR** | 4 | ~4 days | 6-8 hours | ~4.5 days | $10-12 |
| **W-REINFORCE** | 4 | ~4 days | 6-8 hours | ~4.5 days | $10-12 |

**Total for all 4 methods:** ~15.5 days (~$35-43 if run sequentially)

---

### Paper Configuration (34K samples, 4 epochs)

| Method | Generations | Training Time | Eval Time | Total Time | Cost (Colab Pro) |
|--------|------------|---------------|-----------|------------|------------------|
| **GRPO** | 4 | ~12 days | 6-8 hours | ~12.5 days | $30-35 |
| **PSR** | 4 | ~10 days | 6-8 hours | ~10.5 days | $25-30 |
| **NSR** | 4 | ~16 days | 6-8 hours | ~16.5 days | $40-45 |
| **W-REINFORCE** | 4 | ~16 days | 6-8 hours | ~16.5 days | $40-45 |

**Total for all 4 methods:** ~56 days (~$135-155 if run sequentially)

---

## üíæ Checkpointing & Resume

Colab may disconnect after 12-24 hours. Use checkpointing to resume.

### Save Checkpoint to Google Drive

```python
# Cell 16: Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Create checkpoint directory
!mkdir -p /content/drive/MyDrive/chartrl_checkpoints

# Sync checkpoints to Drive during training (run in background)
import time
import shutil
import os

def sync_checkpoints():
    while True:
        time.sleep(3600)  # Every hour
        if os.path.exists('grpo-start-ckpts'):
            print("Syncing checkpoints to Drive...")
            shutil.copytree('grpo-start-ckpts',
                          '/content/drive/MyDrive/chartrl_checkpoints/grpo-start-ckpts',
                          dirs_exist_ok=True)
            print("‚úì Synced")

# Run in background
import threading
sync_thread = threading.Thread(target=sync_checkpoints, daemon=True)
sync_thread.start()
```

### Resume from Checkpoint

```python
# Cell 17: Resume training after disconnect
# Copy checkpoints back from Drive
!cp -r /content/drive/MyDrive/chartrl_checkpoints/grpo-start-ckpts .

# Re-run same training command - will resume automatically
!accelerate launch --config_file=deepspeed_zero3.yaml main.py \
  --mode nsr \
  --vlm-name qwen2-5-3b \
  --dataset-name evochart \
  --seed 2025 \
  --subset-size 10000 \
  --num-epochs 1 \
  --num-generations 4
```

---

## üîç Monitoring Training

### Check GPU Usage

```python
# Cell 18: Monitor GPU
!nvidia-smi
```

**Expected output:**
```
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA A100-SXM...  Off  | 00000000:00:04.0 Off |                    0 |
| N/A   42C    P0    58W / 400W |  15234MiB / 40960MiB |     85%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
```

**Good signs:**
- GPU Util: 80-100% (GPU is working)
- Memory: 12-18GB / 40GB (using ~30-45%)
- Temp: <80¬∞C (safe temperature)

**Bad signs:**
- GPU Util: 0% ‚Üí Training crashed
- Memory: >38GB / 40GB ‚Üí May OOM soon
- Temp: >85¬∞C ‚Üí Thermal throttling

### Check Training Progress

```python
# Cell 19: View training logs
!tail -n 50 /root/.local/state/wandb/latest-run/files/output.log
```

Or monitor in Weights & Biases dashboard.

---

## üêõ Troubleshooting

### Issue 1: CUDA Out of Memory

**Error:**
```
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 GiB
```

**Solutions:**

1. **Reduce batch size:**
   ```python
   --batch-size 1  # Instead of 2
   ```

2. **Enable gradient checkpointing:**
   ```python
   # Remove --disable-gradient-checkpointing flag
   !accelerate launch --config_file=deepspeed_zero3.yaml main.py \
     --mode nsr \
     ... \
     # Don't include --disable-gradient-checkpointing
   ```

3. **Reduce sequence lengths:**
   Edit `main.py` lines 666-669:
   ```python
   max_prompt_length = 2048,  # Reduced from 4096
   max_completion_length = 384,  # Reduced from 768
   ```

---

### Issue 2: Colab Disconnects

**Error:** Colab disconnects after 12 hours

**Solutions:**

1. **Use Colab Pro+** (longer sessions)

2. **Auto-save to Drive** (see Checkpointing section above)

3. **Use keep-alive script:**
   ```javascript
   // Paste in browser console (F12)
   function ClickConnect(){
     console.log("Keeping Colab alive");
     document.querySelector("colab-connect-button").click()
   }
   setInterval(ClickConnect, 60000)
   ```

---

### Issue 3: Training Too Slow

**Problem:** Training taking longer than expected

**Check:**

1. **GPU type:**
   ```python
   !nvidia-smi --query-gpu=name --format=csv
   ```
   Should show: `NVIDIA A100-SXM4-40GB`

   If showing `Tesla T4`: You're on free tier ‚Üí Need Colab Pro!

2. **GPU utilization:**
   ```python
   !nvidia-smi
   ```
   GPU-Util should be 80-100%

3. **Batch size too small:**
   ```python
   --batch-size 2  # Try increasing to 4 if memory allows
   ```

---

### Issue 4: NSR No Negative Samples

**Error:**
```
NSR: Filtered 0/4 negative samples
Warning: No negative samples found
```

**Cause:** Model is generating mostly correct samples (good problem to have!)

**Solutions:**

1. **Increase generations:**
   ```python
   --num-generations 8  # Instead of 4
   ```

2. **Lower reward threshold:**
   ```python
   --reward-threshold 0.3  # Instead of 0.5
   ```

3. **Use early checkpoint** (less trained ‚Üí more errors)

---

### Issue 5: Import Errors

**Error:**
```
ModuleNotFoundError: No module named 'trainers'
```

**Solution:**

Make sure you're in the correct directory:
```python
%cd /content/chartrl
!ls -la trainers/
```

Should show:
```
trainers/
  psr_trainer.py
  nsr_trainer.py
  weighted_reinforce_trainer.py
```

---

## üìù Complete Colab Notebook Template

Here's a complete notebook you can copy-paste:

```python
# ============================================================================
# CELL 1: Setup
# ============================================================================
# Install dependencies
!pip install -q torch==2.4.0 transformers==4.47.1 peft==0.14.0 trl==0.12.1
!pip install -q accelerate==1.2.1 deepspeed==0.15.4 datasets==3.1.0
!pip install -q qwen-vl-utils==0.0.8 wandb sentence-transformers sacrebleu
!pip install -q matplotlib numpy

# Clone repository (or upload files)
!git clone https://github.com/yourusername/chartrl.git
%cd chartrl

# Create DeepSpeed config
config = """compute_environment: LOCAL_MACHINE
debug: false
deepspeed_config:
  deepspeed_multinode_launcher: standard
  offload_optimizer_device: none
  offload_param_device: none
  zero3_init_flag: true
  zero3_save_16bit_model: true
  zero_stage: 3
distributed_type: DEEPSPEED
downcast_bf16: 'no'
machine_rank: 0
main_training_function: main
mixed_precision: bf16
num_machines: 1
num_processes: 1
rdzv_backend: static
same_network: true
use_cpu: false
main_process_port: 29501
"""
with open('deepspeed_zero3.yaml', 'w') as f:
    f.write(config)

print("‚úì Setup complete!")

# ============================================================================
# CELL 2: Mount Google Drive (for checkpointing)
# ============================================================================
from google.colab import drive
drive.mount('/content/drive')

!mkdir -p /content/drive/MyDrive/chartrl_checkpoints

# ============================================================================
# CELL 3: Train GRPO Baseline
# ============================================================================
!accelerate launch --config_file=deepspeed_zero3.yaml main.py \
  --mode grpo \
  --vlm-name qwen2-5-3b \
  --dataset-name evochart \
  --seed 2025 \
  --subset-size 10000 \
  --num-epochs 1 \
  --num-generations 2 \
  --batch-size 2 \
  --disable-gradient-checkpointing

# ============================================================================
# CELL 4: Train PSR
# ============================================================================
!accelerate launch --config_file=deepspeed_zero3.yaml main.py \
  --mode psr \
  --vlm-name qwen2-5-3b \
  --dataset-name evochart \
  --seed 2025 \
  --subset-size 10000 \
  --num-epochs 1 \
  --num-generations 2 \
  --batch-size 2 \
  --reward-threshold 0.5 \
  --disable-gradient-checkpointing

# ============================================================================
# CELL 5: Train NSR
# ============================================================================
!accelerate launch --config_file=deepspeed_zero3.yaml main.py \
  --mode nsr \
  --vlm-name qwen2-5-3b \
  --dataset-name evochart \
  --seed 2025 \
  --subset-size 10000 \
  --num-epochs 1 \
  --num-generations 4 \
  --batch-size 2 \
  --reward-threshold 0.5 \
  --disable-gradient-checkpointing

# ============================================================================
# CELL 6: Train W-REINFORCE (RECOMMENDED)
# ============================================================================
!accelerate launch --config_file=deepspeed_zero3.yaml main.py \
  --mode w-reinforce \
  --vlm-name qwen2-5-3b \
  --dataset-name evochart \
  --seed 2025 \
  --subset-size 10000 \
  --num-epochs 1 \
  --num-generations 4 \
  --batch-size 2 \
  --lambda-psr 0.1 \
  --reward-threshold 0.5 \
  --disable-gradient-checkpointing

# ============================================================================
# CELL 7: Generate Samples for Pass@k
# ============================================================================
methods = ['grpo', 'psr', 'nsr', 'w-reinforce']

for method in methods:
    suffix = f"-{method}" if method != "grpo" else ""
    checkpoint = f"grpo-start-ckpts/qwen2-5-3b-prm-large-train-v2{suffix}-2025"

    !python generate_samples.py \
      --checkpoint-path {checkpoint} \
      --dataset-name evochart \
      --output-path samples_{method}.json \
      --num-samples 256 \
      --batch-size 4 \
      --max-problems 500

# ============================================================================
# CELL 8: Compute Pass@k
# ============================================================================
for method in methods:
    !python evaluation/pass_at_k.py \
      --samples-path samples_{method}.json \
      --output-path results_{method}.json \
      --method-name {method.upper()}

# ============================================================================
# CELL 9: Plot Comparison
# ============================================================================
!python evaluation/plotting.py \
  --results results_grpo.json results_psr.json results_nsr.json results_w_reinforce.json \
  --labels "GRPO" "PSR" "NSR" "W-REINFORCE" \
  --output comparison_pass_at_k.png \
  --title "Pass@k Comparison: Chart Reasoning"

# Display plot
from IPython.display import Image
Image('comparison_pass_at_k.png')

# ============================================================================
# CELL 10: Download Results
# ============================================================================
from google.colab import files

files.download('results_grpo.json')
files.download('results_psr.json')
files.download('results_nsr.json')
files.download('results_w_reinforce.json')
files.download('comparison_pass_at_k.png')

print("‚úì All results downloaded!")
```

---

## üéØ Recommended Workflow

### Day 1: Quick Validation (1K samples, ~6 hours)

```python
# Test all 4 methods on tiny subset
for mode in ['grpo', 'psr', 'nsr', 'w-reinforce']:
    !accelerate launch --config_file=deepspeed_zero3.yaml main.py \
      --mode {mode} \
      --vlm-name qwen2-5-3b \
      --dataset-name evochart \
      --seed 2025 \
      --subset-size 1000 \
      --num-epochs 1 \
      --num-generations 4 \
      --batch-size 2
```

**Total time:** ~6 hours (1.5 hours √ó 4 methods)
**Cost:** ~$1

---

### Day 2-5: Fast Training (10K samples, ~100 hours)

Run each method in separate Colab notebook (parallel execution):

**Notebook 1:** GRPO (~21 hours)
**Notebook 2:** PSR (~18 hours)
**Notebook 3:** NSR (~24 hours)
**Notebook 4:** W-REINFORCE (~24 hours)

**Total time:** ~24 hours (if parallel)
**Cost:** ~$12 (4 notebooks √ó $3)

---

### Day 6: Evaluation (~12 hours)

```python
# Generate samples for all 4 methods (~8 hours)
# Compute Pass@k (~10 minutes)
# Plot curves (~1 minute)
# Analyze results
```

**Total time:** ~12 hours
**Cost:** ~$2

---

## üìä Expected Results

After running all 4 methods, you should see Pass@k curves like:

```
Method        Pass@1   Pass@8   Pass@64  Pass@256
GRPO          65.1%    72.1%    76.8%    78.9%
PSR           67.3%    71.2%    74.5%    76.1%
NSR           65.1%    73.5%    78.2%    80.7%
W-REINFORCE   68.1%    74.2%    79.5%    81.3%
```

**Key findings:**
- PSR has best Pass@1 but worst Pass@256
- NSR matches GRPO at Pass@1 but beats it at Pass@256
- W-REINFORCE is best overall

---

## üí∞ Total Cost Estimate

| Configuration | Time | Cost (Sequential) | Cost (Parallel) |
|---------------|------|-------------------|-----------------|
| **Quick Test (1K)** | 6 hours | $1 | $4 (4 notebooks) |
| **Fast (10K)** | 100 hours | $12 | $12 (4 notebooks) |
| **Moderate (34K)** | 15.5 days | $40 | $40 (4 notebooks) |
| **Paper (34K, 4 epochs)** | 56 days | $150 | $150 (4 notebooks) |

**Recommendation:** Start with **Fast (10K)** configuration for $12-15 total cost.

---

## üöÄ Next Steps

1. ‚úÖ Run quick test (1K samples) to validate setup
2. ‚úÖ Run fast training (10K samples) for all 4 methods
3. ‚úÖ Evaluate Pass@k and plot curves
4. ‚úÖ Analyze which method works best for your use case
5. ‚≠ê Scale to full 34K if needed

---

## Summary

**Complete Colab setup:** 5 minutes
**Fast training (10K):** ~24 hours (parallel) or ~100 hours (sequential)
**Evaluation:** ~12 hours
**Total cost:** ~$12-15 (Colab Pro)

**Expected outcome:** Pass@k curves showing NSR and W-REINFORCE preserve diversity better than GRPO while maintaining accuracy!

# NSR Training - Complete Execution Guide

**ONE comprehensive guide with all commands and step-by-step Colab cells.**

---

## ðŸ“‹ Table of Contents

1. [Prerequisites](#prerequisites)
2. [Colab Setup (Step-by-Step Cells)](#colab-setup-step-by-step-cells)
3. [POC Test - 100 Samples](#poc-test---100-samples)
4. [Fast Training - 10K Samples](#fast-training---10k-samples)
5. [Full Training - 34K Samples](#full-training---34k-samples)
6. [Monitoring & Visualization](#monitoring--visualization)
7. [Pass@k Evaluation](#passk-evaluation)
8. [Troubleshooting](#troubleshooting)

---

## Prerequisites

### Required

- **Google Colab Pro** ($10-20/month) for A100 GPU
- **Hugging Face account** (free)
- **Weights & Biases account** (free, optional but recommended)

### Optional

- **Google Drive** for checkpointing
- **GitHub account** for code repository

---

## Colab Setup (Step-by-Step Cells)

### Cell 1: GPU Check and Setup (30 seconds)

```python
# Check GPU type
!nvidia-smi --query-gpu=name --format=csv,noheader

# Should output: NVIDIA A100-SXM4-40GB
# If you see Tesla T4, you need Colab Pro!

# Set environment
import os
os.environ['HF_HUB_CACHE'] = '/content/hf_cache'
os.environ['TRANSFORMERS_CACHE'] = '/content/hf_cache'
os.environ['HF_HOME'] = '/content/hf_cache'

print("âœ“ GPU check complete")
```

**Expected output:**
```
NVIDIA A100-SXM4-40GB
âœ“ GPU check complete
```

---

### Cell 2: Install Dependencies (2-3 minutes)

```python
# Install all required packages
!pip install -q torch==2.4.0
!pip install -q transformers==4.47.1
!pip install -q peft==0.14.0
!pip install -q trl==0.12.1
!pip install -q accelerate==1.2.1
!pip install -q deepspeed==0.15.4
!pip install -q datasets==3.1.0
!pip install -q qwen-vl-utils==0.0.8
!pip install -q wandb
!pip install -q sentence-transformers
!pip install -q sacrebleu
!pip install -q matplotlib
!pip install -q numpy
!pip install -q pandas
!pip install -q pillow

print("âœ“ All dependencies installed")
```

**Expected output:**
```
[Installation logs...]
âœ“ All dependencies installed
```

---

### Cell 3: Upload Code Files (1 minute)

**Option A: Clone from GitHub (Recommended)**

```python
# Clone your repository
!git clone https://github.com/yourusername/chartrl.git
%cd chartrl

print("âœ“ Code cloned from GitHub")
```

**Option B: Upload Files Manually**

```python
from google.colab import files
import os

# Create directory structure
!mkdir -p chartrl
%cd chartrl
!mkdir -p trainers
!mkdir -p evaluation

print("Upload these files using the file browser (left sidebar):")
print("Required files:")
print("  - main.py")
print("  - grpo_utils.py")
print("  - dataset_process.py")
print("  - models.py")
print("  - prompts.py")
print("  - metrics.py")
print("  - utils.py")
print("  - deepspeed_zero3.yaml")
print("  - trainers/psr_trainer.py")
print("  - trainers/nsr_trainer.py")
print("  - trainers/weighted_reinforce_trainer.py")
print("  - evaluation/pass_at_k.py")
print("  - evaluation/plotting.py")
print("  - generate_samples.py")
print("  - monitor_training.py")
```

---

### Cell 4: Create Module Init Files (10 seconds)

```python
# Create __init__.py files for Python modules
!touch trainers/__init__.py
!touch evaluation/__init__.py

# Verify structure
!ls -la trainers/
!ls -la evaluation/

print("âœ“ Module structure created")
```

**Expected output:**
```
trainers/:
__init__.py
psr_trainer.py
nsr_trainer.py
weighted_reinforce_trainer.py

evaluation/:
__init__.py
pass_at_k.py
plotting.py

âœ“ Module structure created
```

---

### Cell 5: Create DeepSpeed Config (10 seconds)

```python
# Create DeepSpeed Zero-3 configuration
deepspeed_config = """compute_environment: LOCAL_MACHINE
debug: false
deepspeed_config:
  deepspeed_multinode_launcher: standard
  offload_optimizer_device: none
  offload_param_device: none
  zero3_init_flag: true
  zero3_save_16bit_model: true
  zero_stage: 3
distributed_type: DEEPSPEED
downcast_bf16: 'no'
machine_rank: 0
main_training_function: main
mixed_precision: bf16
num_machines: 1
num_processes: 1
rdzv_backend: static
same_network: true
tpu_env: []
tpu_use_cluster: false
tpu_use_sudo: false
use_cpu: false
main_process_port: 29501
"""

with open('deepspeed_zero3.yaml', 'w') as f:
    f.write(deepspeed_config)

print("âœ“ DeepSpeed config created")
```

---

### Cell 6: Login to Weights & Biases (1 minute, optional)

```python
import wandb

# Login to W&B for experiment tracking
wandb.login()

# Or to disable W&B:
# import os
# os.environ['WANDB_MODE'] = 'disabled'

print("âœ“ Weights & Biases configured")
```

**You'll be prompted to enter your W&B API key (get from https://wandb.ai/authorize)**

---

### Cell 7: Mount Google Drive (30 seconds, optional)

```python
from google.colab import drive

# Mount Google Drive for checkpoint backup
drive.mount('/content/drive')

# Create backup directory
!mkdir -p /content/drive/MyDrive/chartrl_checkpoints

print("âœ“ Google Drive mounted")
```

---

### Cell 8: Setup Auto-Backup (10 seconds, optional)

```python
import time
import shutil
import threading
import os

def backup_checkpoints():
    """Auto-backup checkpoints to Google Drive every hour"""
    while True:
        time.sleep(3600)  # Every 1 hour
        if os.path.exists('grpo-start-ckpts'):
            print(f"\n[{time.strftime('%H:%M:%S')}] Backing up checkpoints to Drive...")
            try:
                shutil.copytree(
                    'grpo-start-ckpts',
                    '/content/drive/MyDrive/chartrl_checkpoints/grpo-start-ckpts',
                    dirs_exist_ok=True
                )
                print(f"[{time.strftime('%H:%M:%S')}] âœ“ Backup complete")
            except Exception as e:
                print(f"[{time.strftime('%H:%M:%S')}] Backup failed: {e}")

# Start backup thread
backup_thread = threading.Thread(target=backup_checkpoints, daemon=True)
backup_thread.start()

print("âœ“ Auto-backup started (saves every hour)")
```

---

## POC Test - 100 Samples

**Purpose:** Validate NSR integration works (30 min, $0.10)

### Cell 9: POC Test - NSR Mode (30 minutes)

```python
# Test NSR mode on 100 samples
!accelerate launch --config_file=deepspeed_zero3.yaml main.py \
  --mode nsr \
  --vlm-name qwen2-5-3b \
  --dataset-name evochart \
  --seed 2025 \
  --subset-size 100 \
  --num-epochs 1 \
  --num-generations 4 \
  --batch-size 2 \
  --disable-gradient-checkpointing \
  2>&1 | tee poc_nsr.log

print("\n" + "="*80)
print("POC Test Complete!")
print("="*80)
```

**Expected output:**
```
NSR TRAINING CONFIGURATION:
  Training mode: nsr
  Training samples: 100
  Epochs: 1
  Batch size: 2
  Generations per sample: 4
  Reward threshold: 0.5

âœ“ NSR reward filtering enabled
  â†’ Training only on NEGATIVE samples (incorrect responses)

trainable params: 18,576,384 || trainable%: 0.4923

Training:
NSR [Step 0]: 2/4 negative samples | Avg reward: -2.145
NSR [Step 10]: 3/4 negative samples | Avg reward: -1.892
...
Training progress: 100% 50/50 [30:00<00:00, 36.00s/it]

âœ“ Model saved to grpo-start-ckpts/qwen2-5-3b-prm-large-train-v2-nsr-2025/
```

---

### Cell 10: Verify POC Results (10 seconds)

```python
import os

print("Verification Checklist:")
print("="*80)

# Check checkpoint exists
checkpoint_dir = "grpo-start-ckpts/qwen2-5-3b-prm-large-train-v2-nsr-2025"
if os.path.exists(checkpoint_dir):
    print("âœ“ Checkpoint directory exists")

    # Check for required files
    required_files = [
        "adapter_model.safetensors",
        "adapter_config.json",
        "trainer_state.json",
        "logs"
    ]

    for file in required_files:
        path = os.path.join(checkpoint_dir, file)
        if os.path.exists(path):
            print(f"  âœ“ {file}")
        else:
            print(f"  âœ— {file} MISSING!")
else:
    print("âœ— Checkpoint directory NOT FOUND!")

# Check logs
!echo "\nRecent log entries:"
!grep "NSR \[Step" poc_nsr.log | tail -5

print("\n" + "="*80)
print("If all checks pass, NSR integration is working!")
print("="*80)
```

---

### Cell 11: Monitor POC Training (30 seconds)

```python
# Generate convergence plots
!python monitor_training.py \
  --checkpoint-dir grpo-start-ckpts/qwen2-5-3b-prm-large-train-v2-nsr-2025 \
  --output-dir poc_plots

# Display plots
from IPython.display import Image, display

print("\n" + "="*80)
print("Training Convergence Plots")
print("="*80)

plots = [
    ("Training Loss", "poc_plots/training_loss.png"),
    ("Learning Rate", "poc_plots/learning_rate.png"),
    ("Gradient Norm", "poc_plots/gradient_norm.png"),
]

for title, path in plots:
    if os.path.exists(path):
        print(f"\n{title}:")
        display(Image(path))
```

---

## Fast Training - 10K Samples

**Purpose:** Full experiment (20 hours, $3.50)

### Cell 12: Train All 4 Modes - GRPO (20 hours)

```python
# GRPO Baseline
!accelerate launch --config_file=deepspeed_zero3.yaml main.py \
  --mode grpo \
  --vlm-name qwen2-5-3b \
  --dataset-name evochart \
  --seed 2025 \
  --subset-size 10000 \
  --num-epochs 1 \
  --num-generations 2 \
  --batch-size 2 \
  --disable-gradient-checkpointing \
  2>&1 | tee train_grpo.log

print("\nâœ“ GRPO training complete")
```

**Time:** ~18-21 hours
**Cost:** ~$2.50

---

### Cell 13: Train All 4 Modes - PSR (18 hours)

```python
# PSR - Positive Sample Reinforcement
!accelerate launch --config_file=deepspeed_zero3.yaml main.py \
  --mode psr \
  --vlm-name qwen2-5-3b \
  --dataset-name evochart \
  --seed 2025 \
  --subset-size 10000 \
  --num-epochs 1 \
  --num-generations 2 \
  --batch-size 2 \
  --reward-threshold 0.5 \
  --disable-gradient-checkpointing \
  2>&1 | tee train_psr.log

print("\nâœ“ PSR training complete")
```

**Time:** ~16-18 hours
**Cost:** ~$2.00

---

### Cell 14: Train All 4 Modes - NSR (24 hours)

```python
# NSR - Negative Sample Reinforcement
!accelerate launch --config_file=deepspeed_zero3.yaml main.py \
  --mode nsr \
  --vlm-name qwen2-5-3b \
  --dataset-name evochart \
  --seed 2025 \
  --subset-size 10000 \
  --num-epochs 1 \
  --num-generations 4 \
  --batch-size 2 \
  --reward-threshold 0.5 \
  --disable-gradient-checkpointing \
  2>&1 | tee train_nsr.log

print("\nâœ“ NSR training complete")
```

**Time:** ~22-24 hours
**Cost:** ~$3.50

---

### Cell 15: Train All 4 Modes - W-REINFORCE (24 hours, BEST)

```python
# W-REINFORCE - Weighted combination (RECOMMENDED)
!accelerate launch --config_file=deepspeed_zero3.yaml main.py \
  --mode w-reinforce \
  --vlm-name qwen2-5-3b \
  --dataset-name evochart \
  --seed 2025 \
  --subset-size 10000 \
  --num-epochs 1 \
  --num-generations 4 \
  --batch-size 2 \
  --lambda-psr 0.1 \
  --reward-threshold 0.5 \
  --disable-gradient-checkpointing \
  2>&1 | tee train_w_reinforce.log

print("\nâœ“ W-REINFORCE training complete")
```

**Time:** ~22-24 hours
**Cost:** ~$3.50

---

## Full Training - 34K Samples

**Purpose:** Full dataset (3 days per mode, $10/mode)

### Cell 16: Full Training (3 days per mode)

```python
# Full training on 34K samples (remove --subset-size)
# Choose one mode or run sequentially

MODE = "w-reinforce"  # Change to: grpo, psr, nsr, or w-reinforce

!accelerate launch --config_file=deepspeed_zero3.yaml main.py \
  --mode {MODE} \
  --vlm-name qwen2-5-3b \
  --dataset-name evochart \
  --seed 2025 \
  --num-epochs 1 \
  --num-generations 4 \
  --batch-size 2 \
  --lambda-psr 0.1 \
  --reward-threshold 0.5 \
  --disable-gradient-checkpointing \
  2>&1 | tee train_full_{MODE}.log

print(f"\nâœ“ Full {MODE.upper()} training complete")
```

**Time:** ~3 days per mode
**Cost:** ~$10 per mode

---

## Monitoring & Visualization

### Cell 17: Real-time Monitoring (During Training)

```python
# Check training progress
!tail -20 train_nsr.log

# Or for specific metrics
!grep "NSR \[Step" train_nsr.log | tail -10

# Check GPU usage
!nvidia-smi

# View wandb dashboard
import wandb
print("\nView real-time metrics at:")
print("https://wandb.ai/chartrl/chartrl-nsr")
```

---

### Cell 18: Generate Comparison Plots (After All Training)

```python
# Compare all 4 training modes
!python monitor_training.py \
  --compare \
  --checkpoints \
    grpo-start-ckpts/qwen2-5-3b-prm-large-train-v2-2025 \
    grpo-start-ckpts/qwen2-5-3b-prm-large-train-v2-psr-2025 \
    grpo-start-ckpts/qwen2-5-3b-prm-large-train-v2-nsr-2025 \
    grpo-start-ckpts/qwen2-5-3b-prm-large-train-v2-w-reinforce-2025 \
  --labels GRPO PSR NSR "W-REINFORCE" \
  --output-dir comparison_plots

print("\nâœ“ Comparison plots generated")
```

---

### Cell 19: Display Comparison Plots

```python
from IPython.display import Image, display
import os

plots = {
    "Training Loss Comparison": "comparison_plots/training_loss.png",
    "Learning Rate Schedule": "comparison_plots/learning_rate.png",
    "Gradient Norm": "comparison_plots/gradient_norm.png",
    "Rewards Progression": "comparison_plots/rewards.png",
}

print("="*80)
print("TRAINING COMPARISON PLOTS")
print("="*80)

for title, path in plots.items():
    if os.path.exists(path):
        print(f"\n{title}:")
        display(Image(path, width=800))
    else:
        print(f"\n{title}: Not found")
```

---

## Pass@k Evaluation

### Cell 20: Generate Samples for Pass@k (2-4 hours per mode)

```python
# Generate 256 samples per problem for Pass@k evaluation
# Do this for each trained mode

modes = ["grpo", "psr", "nsr", "w-reinforce"]

for mode in modes:
    print(f"\n{'='*80}")
    print(f"Generating samples for {mode.upper()}")
    print('='*80)

    suffix = f"-{mode}" if mode != "grpo" else ""
    checkpoint_path = f"grpo-start-ckpts/qwen2-5-3b-prm-large-train-v2{suffix}-2025"

    !python generate_samples.py \
      --checkpoint-path {checkpoint_path} \
      --dataset-name evochart \
      --output-path samples_{mode}.json \
      --num-samples 256 \
      --batch-size 4 \
      --max-problems 500

    print(f"\nâœ“ {mode.upper()} samples generated")

print("\n" + "="*80)
print("All samples generated!")
print("="*80)
```

**Time:** ~2-4 hours per mode
**Total:** ~8-16 hours for all 4 modes

---

### Cell 21: Compute Pass@k Metrics (1 minute per mode)

```python
# Compute Pass@k for all modes
modes = ["grpo", "psr", "nsr", "w-reinforce"]

for mode in modes:
    !python evaluation/pass_at_k.py \
      --samples-path samples_{mode}.json \
      --output-path results_{mode}.json \
      --method-name {mode.upper()}

print("\nâœ“ Pass@k computed for all modes")
```

---

### Cell 22: Display Pass@k Results

```python
import json
import pandas as pd

# Load all results
results = {}
for mode in ["grpo", "psr", "nsr", "w-reinforce"]:
    with open(f"results_{mode}.json", 'r') as f:
        data = json.load(f)
        results[mode.upper()] = data['pass_at_k']

# Create comparison table
df = pd.DataFrame(results).T
df = df * 100  # Convert to percentage
df = df.round(2)

print("\n" + "="*80)
print("PASS@K COMPARISON TABLE")
print("="*80)
print(df.to_string())
print("="*80)

# Highlight best values
print("\nBest Performance:")
for k in df.columns:
    best_method = df[k].idxmax()
    best_value = df[k].max()
    print(f"  Pass@{k}: {best_method} ({best_value:.2f}%)")
```

**Expected output:**
```
PASS@K COMPARISON TABLE
================================================================================
              1      2      4      8     16     32     64    128    256
GRPO      65.14  69.23  73.45  76.12  78.34  79.89  81.23  82.34  83.12
PSR       67.23  70.12  73.01  74.89  76.23  77.12  78.01  78.89  79.23
NSR       65.23  70.45  74.89  78.23  80.45  82.12  83.67  84.89  85.67
W-REINFORCE 68.12  71.34  75.67  79.45  81.67  83.34  84.89  86.12  87.23
================================================================================

Best Performance:
  Pass@1: W-REINFORCE (68.12%)
  Pass@256: W-REINFORCE (87.23%)
```

---

### Cell 23: Plot Pass@k Curves

```python
# Generate Pass@k comparison plot
!python evaluation/plotting.py \
  --results results_grpo.json results_psr.json results_nsr.json results_w_reinforce.json \
  --labels GRPO PSR NSR "W-REINFORCE" \
  --output pass_at_k_comparison.png \
  --title "Pass@k Comparison: NSR vs Baselines"

# Display plot
from IPython.display import Image
display(Image('pass_at_k_comparison.png', width=1000))

print("\nâœ“ Pass@k curves plotted")
```

---

## Troubleshooting

### Cell 24: Debug - Check Logs

```python
# Check for errors in logs
import os

log_files = [
    "poc_nsr.log",
    "train_grpo.log",
    "train_psr.log",
    "train_nsr.log",
    "train_w_reinforce.log"
]

print("Checking logs for errors...")
print("="*80)

for log_file in log_files:
    if os.path.exists(log_file):
        print(f"\n{log_file}:")
        # Check for common errors
        !grep -i "error\|exception\|failed" {log_file} | tail -5
    else:
        print(f"\n{log_file}: Not found")

print("\n" + "="*80)
```

---

### Cell 25: Debug - Check Checkpoints

```python
# Verify all checkpoints exist
import os

expected_checkpoints = [
    "grpo-start-ckpts/qwen2-5-3b-prm-large-train-v2-2025",
    "grpo-start-ckpts/qwen2-5-3b-prm-large-train-v2-psr-2025",
    "grpo-start-ckpts/qwen2-5-3b-prm-large-train-v2-nsr-2025",
    "grpo-start-ckpts/qwen2-5-3b-prm-large-train-v2-w-reinforce-2025",
]

print("Checkpoint Status:")
print("="*80)

for checkpoint_dir in expected_checkpoints:
    mode = checkpoint_dir.split('-')[-2] if 'psr' in checkpoint_dir or 'nsr' in checkpoint_dir or 'reinforce' in checkpoint_dir else 'grpo'

    if os.path.exists(checkpoint_dir):
        # Check size
        size = !du -sh {checkpoint_dir}
        print(f"âœ“ {mode.upper()}: {size[0]}")

        # Check required files
        required = ["adapter_model.safetensors", "trainer_state.json"]
        for file in required:
            path = os.path.join(checkpoint_dir, file)
            if not os.path.exists(path):
                print(f"  âœ— Missing: {file}")
    else:
        print(f"âœ— {mode.upper()}: Not found")

print("="*80)
```

---

### Cell 26: Debug - NSR No Negative Samples

```python
# If NSR shows "0/4 negative samples"
# Run this to diagnose

!grep "NSR \[Step" train_nsr.log | head -20

print("\nIf you see '0/4 negative samples', try:")
print("1. Increase --num-generations to 8")
print("2. Lower --reward-threshold to 0.3")
print("3. Use earlier checkpoint (less trained = more errors)")
```

---

### Cell 27: Download Results

```python
from google.colab import files
import shutil

# Create archive of all results
!mkdir -p download_package
!cp -r comparison_plots download_package/
!cp results_*.json download_package/
!cp pass_at_k_comparison.png download_package/
!cp train_*.log download_package/

# Create tar archive
!tar -czf nsr_results.tar.gz download_package/

# Download
files.download('nsr_results.tar.gz')

print("\nâœ“ Results downloaded!")
print("Contains:")
print("  - All Pass@k results (JSON)")
print("  - Comparison plots")
print("  - Pass@k curves")
print("  - Training logs")
```

---

### Cell 28: Download Checkpoints (Optional - Large Files)

```python
from google.colab import files

# Choose which checkpoint to download
MODE = "w-reinforce"  # grpo, psr, nsr, or w-reinforce

suffix = f"-{MODE}" if MODE != "grpo" else ""
checkpoint_dir = f"grpo-start-ckpts/qwen2-5-3b-prm-large-train-v2{suffix}-2025"

# Create tar archive
!tar -czf checkpoint_{MODE}.tar.gz {checkpoint_dir}

# Download
files.download(f'checkpoint_{MODE}.tar.gz')

print(f"\nâœ“ {MODE.upper()} checkpoint downloaded!")
```

---

## Quick Reference

### All Training Commands Summary

```python
# POC (100 samples, 30 min)
!accelerate launch --config_file=deepspeed_zero3.yaml main.py --mode nsr --vlm-name qwen2-5-3b --dataset-name evochart --seed 2025 --subset-size 100 --num-epochs 1 --num-generations 4 --batch-size 2 --disable-gradient-checkpointing

# Fast (10K samples, 20 hours)
!accelerate launch --config_file=deepspeed_zero3.yaml main.py --mode nsr --vlm-name qwen2-5-3b --dataset-name evochart --seed 2025 --subset-size 10000 --num-epochs 1 --num-generations 4 --batch-size 2 --disable-gradient-checkpointing

# Full (34K samples, 3 days)
!accelerate launch --config_file=deepspeed_zero3.yaml main.py --mode nsr --vlm-name qwen2-5-3b --dataset-name evochart --seed 2025 --num-epochs 1 --num-generations 4 --batch-size 2 --disable-gradient-checkpointing
```

---

### Training Mode Parameters

| Mode | --mode | --num-generations | --lambda-psr | Time (10K) |
|------|--------|-------------------|--------------|------------|
| GRPO | grpo | 2 | - | 18-21h |
| PSR | psr | 2 | - | 16-18h |
| NSR | nsr | 4 | - | 22-24h |
| W-REINFORCE | w-reinforce | 4 | 0.1 | 22-24h |

---

### Expected Results (10K samples)

| Method | Pass@1 | Pass@8 | Pass@64 | Pass@256 |
|--------|--------|--------|---------|----------|
| GRPO | 65% | 72% | 77% | 79% |
| PSR | 67% | 71% | 74% | 76% |
| NSR | 65% | 74% | 78% | 81% |
| W-REINFORCE | 68% | 75% | 80% | 82% |

---

### File Locations

| What | Where |
|------|-------|
| Prompt template | `prompts.py` line 111 |
| Training config | `main.py` lines 669-702 |
| Reward filtering | `main.py` lines 732-793 |
| Checkpoints | `grpo-start-ckpts/` |
| Logs | `grpo-start-ckpts/*/logs/` |
| Results | `results_*.json` |

---

### Time & Cost Summary

| Configuration | Samples | Time | Cost (Colab Pro) |
|---------------|---------|------|------------------|
| **POC** | 100 | 30 min | $0.10 |
| **Quick** | 1K | 2 hours | $0.60 |
| **Fast** | 10K | 20 hours | $3.50 |
| **Full** | 34K | 3 days | $10.00 |

**All 4 modes (10K):** ~86 hours sequential, ~24 hours parallel, ~$12 total

---

## Execution Checklist

**Before starting:**
- [ ] Colab Pro subscription active
- [ ] GPU set to A100 (Runtime â†’ Change runtime type)
- [ ] All code files uploaded or cloned
- [ ] `trainers/__init__.py` created
- [ ] `evaluation/__init__.py` created
- [ ] Wandb account setup (optional)
- [ ] Google Drive mounted (optional)

**POC Test (30 min):**
- [ ] Run Cell 9 (NSR POC)
- [ ] Run Cell 10 (Verify results)
- [ ] Run Cell 11 (Monitor convergence)
- [ ] Confirm: Loss decreased, checkpoints saved, logs show filtering

**Fast Training (1 day):**
- [ ] Run Cells 12-15 (all 4 modes in parallel or sequential)
- [ ] Monitor with Cell 17
- [ ] Generate comparison plots with Cell 18

**Pass@k Evaluation (12 hours):**
- [ ] Run Cell 20 (Generate samples)
- [ ] Run Cell 21 (Compute Pass@k)
- [ ] Run Cell 22 (Display results)
- [ ] Run Cell 23 (Plot curves)

**Download Results:**
- [ ] Run Cell 27 (Download results package)
- [ ] Run Cell 28 (Download checkpoints if needed)

---

## Summary

**This guide provides:**
- âœ… Complete step-by-step Colab setup
- âœ… All training commands (POC, Fast, Full)
- âœ… Monitoring and visualization
- âœ… Pass@k evaluation pipeline
- âœ… Troubleshooting helpers
- âœ… Download utilities

**Total execution time:**
- POC: ~30 minutes ($0.10)
- Fast (all 4 modes): ~24 hours parallel ($12)
- Pass@k evaluation: ~12 hours ($2)
- **Total: ~36 hours, ~$14**

**Everything in one document - just copy-paste cells in order!**
